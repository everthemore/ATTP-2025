{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db7abeb7-df2c-4003-b8e7-cafb81d09743",
   "metadata": {},
   "source": [
    "# Policy gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04e6bc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.13/site-packages (2.9.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.13/site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: gymnasium in /opt/anaconda3/lib/python3.13/site-packages (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/anaconda3/lib/python3.13/site-packages (from gymnasium) (2.1.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from gymnasium) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/anaconda3/lib/python3.13/site-packages (from gymnasium) (0.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0042fcbe-5a70-4ccf-98c7-045e118bc8cc",
   "metadata": {},
   "source": [
    "From here on, we will do everything in Gymnasium format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41161d73-0e40-4e25-a621-5c7b69ed49d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Observation: [0. 0. 0.]\n",
      "Step: [-0.5 -0.5 -0.5] -0.75 False {}\n",
      "Step: [-1. -1. -1.] -3.0 False {}\n",
      "Step: [-1.5 -1.5 -1.5] -6.75 False {}\n",
      "Step: [-2. -2. -2.] -12.0 False {}\n",
      "Step: [-2.5 -2.5 -2.5] -18.75 False {}\n",
      "Step: [-3. -3. -3.] -27.0 False {}\n",
      "Step: [-3.5 -3.5 -3.5] -36.75 False {}\n",
      "Step: [-4. -4. -4.] -48.0 False {}\n",
      "Step: [-4.5 -4.5 -4.5] -60.75 False {}\n",
      "Step: [-5. -5. -5.] -75.0 False {}\n",
      "Step: [-5.5 -5.5 -5.5] -90.75 False {}\n",
      "Step: [-6. -6. -6.] -108.0 True {}\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A custom Gymnasium environment skeleton.\n",
    "    \"\"\"\n",
    "    metadata = {\"render.modes\": [\"human\"]}  # Optional, define render modes\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define action and observation spaces\n",
    "        # Example: Discrete action space with 2 actions (0, 1)\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        \n",
    "        # Example: Continuous observation space (1D array of size 3)\n",
    "        self.observation_space = spaces.Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32)\n",
    "        \n",
    "        # Initialize environment state\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Reset the environment to an initial state.\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Example: Initialize the state\n",
    "        self.state = np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        \n",
    "        # Return the initial observation, and a possible dictionary with information\n",
    "        return self.state, {'message':'successfully reset!', 'hello':'world'}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Perform one step in the environment.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Update the environment state (example logic)\n",
    "        self.state = self.state + action - 0.5  # Dummy dynamics\n",
    "        \n",
    "        # Compute the reward (example logic)\n",
    "        reward = -np.sum(np.square(self.state))  \n",
    "        \n",
    "        # Check if the episode is terminated (example logic) -> reach terminal state of MDP\n",
    "        terminated = np.linalg.norm(self.state) > 10.0  \n",
    "\n",
    "        # Flag if the episode was truncated -> terminated not because reaching MDP endpoint, but e.g. because too many steps taken\n",
    "        truncated = False \n",
    "        \n",
    "        # Additional info (can be empty)\n",
    "        info = {}\n",
    "        \n",
    "        return self.state, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"\n",
    "        Render the environment (optional).\n",
    "        \"\"\"\n",
    "        print(f\"State: {self.state}\")\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Clean up resources (optional).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "#Example usage:\n",
    "env = CustomEnv()\n",
    "obs, info = env.reset()\n",
    "print(\"Initial Observation:\", obs)\n",
    "action = env.action_space.sample()\n",
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "done = terminated or truncated\n",
    "print(\"Step:\", obs, reward, done, info)\n",
    "\n",
    "while not done:\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    print(\"Step:\", obs, reward, done, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeab3a3-e734-4031-af74-5a52a28e050c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5488269-b143-4cee-a25a-7124c2a3f0cb",
   "metadata": {},
   "source": [
    "## Another standard benchmark: the cartpole problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f8bfdf3-69f8-44de-bce8-bc80190e5de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State dimension: 4, Action dimension: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Set up the CartPole environment\n",
    "env = gym.make(\"CartPole-v1\")#, render_mode=\"human\")#rgb_array\")\n",
    "\n",
    "# Check state and action space\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "print(f\"State dimension: {state_dim}, Action dimension: {action_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3aa5cb58-d2a3-4c41-866b-14799c30d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim),\n",
    "            nn.Softmax(dim=-1)  # Output probabilities for actions\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.fc(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b5e1842-079c-49e7-be23-9c1a74b26304",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def select_action(policy, state):\n",
    "    state = torch.tensor(state, dtype=torch.float32)  # Convert state to tensor\n",
    "    action_probs = policy(state)                      # Get action probabilities\n",
    "\n",
    "    # Here is a trick: use the Categorial distribution so we can use '.sample()'\n",
    "    # Without it, i.e. if we just use the softmax, we would use np.random.choice perhaps\n",
    "    # Similarly, Categorical provides the .log_prob(), which we would have to do separately otherwise\n",
    "    # (And finally, Categorical works nicely if we wanted to use batches)\n",
    "    \n",
    "    action_dist = torch.distributions.Categorical(action_probs)  # Categorical distribution\n",
    "    action = action_dist.sample()                    # Sample an action\n",
    "    return action.item(), action_dist.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aaf4ea1f-2fe9-4c05-be98-f628ba99a88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled action: 0, Log-probability: -0.7263385653495789\n"
     ]
    }
   ],
   "source": [
    "# Initialize policy network\n",
    "policy = PolicyNetwork(state_dim, action_dim)\n",
    "\n",
    "# Test action sampling\n",
    "state, info = env.reset()\n",
    "action, log_prob = select_action(policy, state)\n",
    "print(f\"Sampled action: {action}, Log-probability: {log_prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5059c96b-2b9f-4922-b14f-0c358ef42ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectory(policy, env, max_steps=200):\n",
    "    states, actions, log_probs, rewards = [], [], [], []\n",
    "    state, info = env.reset()\n",
    "\n",
    "    # The trajectory can be max_steps long\n",
    "    for _ in range(max_steps):\n",
    "\n",
    "        # For EACH step, we want to store the action and log( pi(a|s) )\n",
    "        action, log_prob = select_action(policy, state)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # Store data\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        done = terminated or truncated\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        # Prepare state for next iteration of for loop\n",
    "        state = next_state\n",
    "    \n",
    "    return states, actions, log_probs, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b10662b9-52e4-4d8f-8d49-9ac4964e466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, gamma=0.99):\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)  # Insert at the beginning\n",
    "    return returns\n",
    "\n",
    "def compute_returns(rewards, gamma=0.99):\n",
    "    returns = np.zeros(len(rewards))\n",
    "    G = 0\n",
    "    for i in range(len(rewards) - 1, -1, -1):  # Iterate from the last reward to the first\n",
    "        G = rewards[i] + gamma * G\n",
    "        returns[i] = G\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a31071-e536-4e8c-a952-85573989b85f",
   "metadata": {},
   "source": [
    "## The REINFORCE update (this is where the gradient is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97ad8de3-55dc-48ca-8910-257b74cc21a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(policy.parameters(), lr=0.01)\n",
    "def reinforce_update(policy, optimizer, log_probs, returns):\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "    # It is a good idea to normalize the returns, but we don't HAVE to\n",
    "    returns = (returns - returns.mean()) / (returns.std() + 1e-8) \n",
    "    \n",
    "    loss = 0\n",
    "    # The gradient is summed over all the steps in a trajectory (hence for loop), and each term has G_t * log_prop(a_t|s_t)\n",
    "    for log_prob, G in zip(log_probs, returns):\n",
    "        loss -= log_prob * G  # Policy gradient loss (negative because we minimize)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a24e203-2ebd-42ea-9b1b-d6127b81a366",
   "metadata": {},
   "source": [
    "## Let's train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f505659-da75-4e50-a8ad-26097fec1997",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: 28.0\n",
      "Episode 100, Total Reward: 26.0\n",
      "Episode 200, Total Reward: 41.0\n",
      "Episode 300, Total Reward: 65.0\n",
      "Episode 400, Total Reward: 57.0\n",
      "Episode 500, Total Reward: 163.0\n",
      "Episode 600, Total Reward: 57.0\n",
      "Episode 700, Total Reward: 69.0\n",
      "Episode 800, Total Reward: 186.0\n",
      "Episode 900, Total Reward: 200.0\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 1000\n",
    "gamma = 0.99\n",
    "\n",
    "reward_progression = []\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    # Collect a single trajectory\n",
    "    states, actions, log_probs, rewards = collect_trajectory(policy, env)\n",
    "    \n",
    "    # Compute the returns for the rewards collected\n",
    "    returns = compute_returns(rewards, gamma)\n",
    "    \n",
    "    # Update policy\n",
    "    reinforce_update(policy, optimizer, log_probs, returns)\n",
    "    \n",
    "    # Logging\n",
    "    total_reward = sum(rewards)\n",
    "    reward_progression.append(total_reward)\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
