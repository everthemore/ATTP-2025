{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78cd85b2-2847-4adc-aa57-b74fe8f3699e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Training the PPO agent...\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 300      |\n",
      "|    ep_rew_mean     | -110     |\n",
      "| time/              |          |\n",
      "|    fps             | 7038     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 378          |\n",
      "|    ep_rew_mean          | -113         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 4745         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057234652 |\n",
      "|    clip_fraction        | 0.0629       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.68        |\n",
      "|    explained_variance   | 0.00921      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 67.3         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00397     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 140          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 473          |\n",
      "|    ep_rew_mean          | -112         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 4309         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062432587 |\n",
      "|    clip_fraction        | 0.0546       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.66        |\n",
      "|    explained_variance   | 0.348        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.44         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0023      |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 80.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 557          |\n",
      "|    ep_rew_mean          | -111         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 4084         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 8            |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0093032755 |\n",
      "|    clip_fraction        | 0.151        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.62        |\n",
      "|    explained_variance   | 0.486        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.358        |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00508     |\n",
      "|    std                  | 0.983        |\n",
      "|    value_loss           | 9.23         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 611         |\n",
      "|    ep_rew_mean          | -111        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3980        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009485258 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.57       |\n",
      "|    explained_variance   | 0.579       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.566       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00418    |\n",
      "|    std                  | 0.967       |\n",
      "|    value_loss           | 12.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 635         |\n",
      "|    ep_rew_mean          | -109        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3908        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010513777 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.53       |\n",
      "|    explained_variance   | 0.254       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.628       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00571    |\n",
      "|    std                  | 0.959       |\n",
      "|    value_loss           | 24.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 625          |\n",
      "|    ep_rew_mean          | -109         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3863         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059337188 |\n",
      "|    clip_fraction        | 0.0599       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.51        |\n",
      "|    explained_variance   | 0.605        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.7         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00363     |\n",
      "|    std                  | 0.959        |\n",
      "|    value_loss           | 23.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 621         |\n",
      "|    ep_rew_mean          | -108        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3828        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006228473 |\n",
      "|    clip_fraction        | 0.0427      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.5        |\n",
      "|    explained_variance   | 0.738       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.53        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00343    |\n",
      "|    std                  | 0.954       |\n",
      "|    value_loss           | 23.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 668         |\n",
      "|    ep_rew_mean          | -107        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3807        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009565456 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.49       |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.56        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00658    |\n",
      "|    std                  | 0.954       |\n",
      "|    value_loss           | 12.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 730          |\n",
      "|    ep_rew_mean          | -105         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3785         |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 21           |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0101852175 |\n",
      "|    clip_fraction        | 0.103        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.47        |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.26         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00506     |\n",
      "|    std                  | 0.947        |\n",
      "|    value_loss           | 6.94         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 748         |\n",
      "|    ep_rew_mean          | -103        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3760        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009994833 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.43       |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.46        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.000726   |\n",
      "|    std                  | 0.939       |\n",
      "|    value_loss           | 6.69        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 824         |\n",
      "|    ep_rew_mean          | -101        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3750        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010828039 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.42       |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.61        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00201    |\n",
      "|    std                  | 0.938       |\n",
      "|    value_loss           | 8.75        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 855         |\n",
      "|    ep_rew_mean          | -99.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3741        |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009001543 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.37       |\n",
      "|    explained_variance   | -0.243      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0479      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00504    |\n",
      "|    std                  | 0.922       |\n",
      "|    value_loss           | 0.593       |\n",
      "-----------------------------------------\n",
      "Training complete!\n",
      "Evaluating the PPO agent...\n",
      "Model saved as 'ppo_lunarlander.zip'.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Step 1: Create the LunarLander-v2 environment\n",
    "env_name = \"BipedalWalker-v3\"\n",
    "env = make_vec_env(env_name, n_envs=4)  # Vectorized environment for parallelization\n",
    "\n",
    "# Step 2: Define the PPO model\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",       # Multi-layer perceptron policy\n",
    "    env,               # Environment\n",
    "    verbose=1,         # Print training information\n",
    "    learning_rate=3e-4,  # Learning rate for the optimizer\n",
    "    n_steps=2048,      # Number of steps to run per environment per update\n",
    "    batch_size=64,     # Minibatch size for gradient updates\n",
    "    n_epochs=10,       # Number of epochs to optimize the surrogate loss\n",
    "    gamma=0.99,        # Discount factor\n",
    "    gae_lambda=0.95,   # GAE (Generalized Advantage Estimation) lambda\n",
    "    clip_range=0.2,    # Clipping parameter for PPO\n",
    ")\n",
    "\n",
    "# Step 3: Train the PPO agent\n",
    "timesteps = 100000  # Total training steps\n",
    "print(\"Training the PPO agent...\")\n",
    "model.learn(total_timesteps=timesteps)\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Step 4: Evaluate the agent\n",
    "def evaluate_agent(model, env, n_episodes=5):\n",
    "    \"\"\"\n",
    "    Evaluate the trained PPO agent in the given environment.\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    for episode in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)  # Use the trained policy\n",
    "            obs, reward, done, _, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "        total_rewards.append(episode_reward)\n",
    "        print(f\"Episode {episode + 1}: Reward = {episode_reward:.2f}\")\n",
    "    avg_reward = sum(total_rewards) / n_episodes\n",
    "    print(f\"Average reward over {n_episodes} episodes: {avg_reward:.2f}\")\n",
    "\n",
    "# Evaluate the trained model\n",
    "print(\"Evaluating the PPO agent...\")\n",
    "#evaluate_agent(model, gym.make(env_name))\n",
    "\n",
    "# Step 5: Save the model\n",
    "model.save(\"ppo_lunarlander\")\n",
    "print(\"Model saved as 'ppo_lunarlander.zip'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd29bf23-a384-45e6-a9aa-ef3c3c30fc98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load the model and continue training (optional)\n",
    "# model = PPO.load(\"ppo_lunarlander\", env=env)\n",
    "# model.learn(total_timesteps=100000)\n",
    "# model.save(\"ppo_lunarlander\")\n",
    "# print(\"Model saved as 'ppo_lunarlander.zip'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9363e7-7f28-4a27-97cc-88e9c7982dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", render_mode=\"human\")  # Enable rendering\n",
    "test_model = PPO.load(\"ppo_lunarlander\")\n",
    "\n",
    "# Step 2: Evaluate the agent\n",
    "obs,_ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    # Step 3: Predict action using the trained model\n",
    "    action, _ = test_model.predict(obs, deterministic=True)  # Use deterministic policy for evaluation\n",
    "    obs, reward, done, _, info = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "# Step 4: Close the environment after evaluation\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7e8f17-c5c4-446e-bfa9-df6d26b7c879",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
